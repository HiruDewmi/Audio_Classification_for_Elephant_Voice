{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Requiremets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torchaudio\n",
    "import torchvision.transforms as transforms\n",
    "import os\n",
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import torchvision.models as models\n",
    "import torch.nn.functional as F\n",
    "import time\n",
    "import librosa\n",
    "from skopt import gp_minimize\n",
    "from skopt.space import Real\n",
    "from skopt.utils import use_named_args\n",
    "\n",
    "# Check for CUDA availability\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from collections import defaultdict\n",
    "\n",
    "\n",
    "class Add(nn.Module):\n",
    "    '''\n",
    "    Adds two tensors and returns the result\n",
    "    '''\n",
    "    def __init__(self,activation=None):\n",
    "        super(Add, self).__init__()\n",
    "        self.activation = activation\n",
    "        self.digital = True\n",
    "        \n",
    "    def forward(self, x):\n",
    "        if len(x) != 2:\n",
    "            print('ERR: Num tensors to add',len(x))\n",
    "            raise\n",
    "#         return torch.stack(x,dim=0).sum(dim=0)\n",
    "        if self.activation is not None:\n",
    "            return self.activation(torch.stack(x,dim=0).sum(dim=0))\n",
    "        else:\n",
    "            return torch.stack(x,dim=0).sum(dim=0)\n",
    "        \n",
    "def model_summary(M, pt_191=False):\n",
    "    \"\"\"\n",
    "    This function provides summary of all the named classes in the model.\n",
    "    Use arguments pt_191=True for pytorch 1.9.1 usage, default pt_191 = False\n",
    "    Returns a dictionary of class names and usage count.\n",
    "    \"\"\"\n",
    "    def zero(): return 0\n",
    "    cdict = defaultdict(zero)\n",
    "    \n",
    "\n",
    "    for n,m in M.named_modules(remove_duplicate=True):\n",
    "        if isinstance(m,nn.Conv2d):\n",
    "            if M.get_submodule(n.rsplit('.',1)[0]).__class__.__name__ == 'CART':\n",
    "                cdict['CART_'+m.__class__.__name__]+=1\n",
    "                \n",
    "            else:\n",
    "                cdict[m.__class__.__name__]+=1\n",
    "                \n",
    "            \n",
    "        elif isinstance(m,(nn.ReLU,Add)) and hasattr(m,'digital'):\n",
    "            if m.digital:\n",
    "                cdict[m.__class__.__name__]+=1\n",
    "                \n",
    "            else:\n",
    "                cdict['CART_'+m.__class__.__name__]+=1\n",
    "                \n",
    "        else:\n",
    "             cdict[m.__class__.__name__]+=1\n",
    "        \n",
    "            \n",
    "    w_size=0        \n",
    "    for p in M.parameters():\n",
    "        w_size+=p.shape.numel()\n",
    "    cdict['Parameters'] = str(w_size/1e6)+'M'   \n",
    "        \n",
    "    return dict(cdict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Class AudioDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AudioDataset(Dataset):\n",
    "    def __init__(self, directory, desired_duration, sample_rate=44100):\n",
    "        self.directory = directory\n",
    "        self.classes = sorted(os.listdir(directory))\n",
    "        self.audio_files = []\n",
    "        self.desired_duration = desired_duration\n",
    "        self.sample_rate=sample_rate\n",
    "\n",
    "        for i, class_name in enumerate(self.classes):\n",
    "            class_path = os.path.join(directory, class_name)\n",
    "            for audio_file in os.listdir(class_path):\n",
    "                self.audio_files.append((os.path.join(class_path, audio_file), i))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.audio_files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        audio_file, label = self.audio_files[idx]\n",
    "        waveform, sample_rate = librosa.load(audio_file, sr=None) \n",
    "        spectrogram = self._compute_spectrogram(waveform, sample_rate)\n",
    "        return spectrogram, label\n",
    "    \n",
    "    def _compute_spectrogram(self,waveform, sample_rate):\n",
    "        if len(waveform) != self.desired_duration * self.sample_rate:\n",
    "            waveform = librosa.resample(waveform, orig_sr=len(waveform), target_sr=self.sample_rate)\n",
    "\n",
    "        if len(waveform) < self.desired_duration * self.sample_rate:\n",
    "            pad_size = self.desired_duration * self.sample_rate - len(waveform)\n",
    "            waveform = np.pad(waveform, (0, pad_size))\n",
    "        elif len(waveform) > self.desired_duration * self.sample_rate:\n",
    "            waveform = waveform[:self.desired_duration * self.sample_rate]\n",
    "\n",
    "        spectrogram = librosa.feature.melspectrogram(y=waveform, sr=sample_rate)\n",
    "        # Convert to decibel scale\n",
    "        spectrogram = librosa.power_to_db(spectrogram, ref=np.max)\n",
    "        # Normalize spectrogram values\n",
    "        \n",
    "        spectrogram = (spectrogram - np.min(spectrogram)) / (np.max(spectrogram) - np.min(spectrogram))\n",
    "        return spectrogram"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define data directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dir = 'data/train'\n",
    "validation_dir = 'data/validate'\n",
    "test_dir = 'data/test'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "desired_duration = 6  # Duration in seconds\n",
    "target_sample_rate = 16000\n",
    "\n",
    "train_dataset = AudioDataset(train_dir, desired_duration=desired_duration,sample_rate=target_sample_rate)\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "validation_dataset = AudioDataset(validation_dir,desired_duration=desired_duration,sample_rate=target_sample_rate)\n",
    "validation_loader = DataLoader(validation_dataset, batch_size=8, shuffle=False)\n",
    "\n",
    "test_dataset = AudioDataset(test_dir, desired_duration=desired_duration,sample_rate=target_sample_rate)\n",
    "test_loader = DataLoader(test_dataset, batch_size=1, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# YAMNET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.init as init\n",
    "\n",
    "def init_weights_he(module):\n",
    "    if isinstance(module, (nn.Conv1d, nn.Linear)):\n",
    "        init.kaiming_normal_(module.weight.data, mode='fan_out', nonlinearity='relu')\n",
    "        if module.bias is not None:\n",
    "            init.constant_(module.bias.data, 0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class YAMNet(nn.Module):\n",
    "    def __init__(self, num_classes, num_samples):\n",
    "        super(YAMNet, self).__init__()\n",
    "        self.num_classes = num_classes\n",
    "        self.num_samples = num_samples\n",
    "\n",
    "        # Define the layers\n",
    "        self.conv1 = nn.Sequential(\n",
    "            nn.Conv1d(128, 32, kernel_size=3, stride=2, padding=1),\n",
    "            nn.BatchNorm1d(32, eps=1e-4),\n",
    "            nn.LeakyReLU(inplace=True)\n",
    "        )\n",
    "        self.separable_conv1 = nn.Sequential(\n",
    "            nn.Conv1d(32, 64, kernel_size=3, stride=1, padding=1, groups=32),\n",
    "            # nn.BatchNorm1d(32, eps=1e-4),\n",
    "            nn.Conv1d(64, 64, kernel_size=1),\n",
    "            nn.BatchNorm1d(64, eps=1e-4),\n",
    "            nn.LeakyReLU(inplace=True)\n",
    "        )\n",
    "        self.separable_conv2 = nn.Sequential(\n",
    "            nn.Conv1d(64, 128, kernel_size=3, stride=2, padding=1, groups=64),\n",
    "            # nn.BatchNorm1d(64, eps=1e-4),\n",
    "            nn.Conv1d(128, 128, kernel_size=1),\n",
    "            nn.BatchNorm1d(128, eps=1e-4),\n",
    "            nn.LeakyReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "        self.separable_conv3 = nn.Sequential(\n",
    "            nn.Conv1d(128, 128, kernel_size=3, stride=1, padding=1, groups=128),\n",
    "            nn.Conv1d(128, 128, kernel_size=1),\n",
    "            nn.BatchNorm1d(128, eps=1e-4),\n",
    "            nn.LeakyReLU(inplace=True)\n",
    "        )\n",
    "        self.separable_conv4 = nn.Sequential(\n",
    "            nn.Conv1d(128, 256, kernel_size=3, stride=2, padding=1, groups=128),\n",
    "            nn.Conv1d(256, 256, kernel_size=1),\n",
    "            nn.BatchNorm1d(256, eps=1e-4),\n",
    "            nn.LeakyReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "        self.separable_conv5 = nn.Sequential(\n",
    "            nn.Conv1d(256, 256, kernel_size=3, stride=1, padding=1, groups=256),\n",
    "            nn.Conv1d(256, 256, kernel_size=1),\n",
    "            nn.BatchNorm1d(256, eps=1e-4),\n",
    "            nn.LeakyReLU(inplace=True)\n",
    "        )\n",
    "        self.separable_conv6 = nn.Sequential(\n",
    "            nn.Conv1d(256, 512, kernel_size=3, stride=2, padding=1, groups=256),\n",
    "            # nn.BatchNorm1d(256),\n",
    "            nn.Conv1d(512, 512, kernel_size=1),\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.LeakyReLU(inplace=True)\n",
    "        )\n",
    "        self.separable_conv7 = nn.Sequential(\n",
    "            nn.Conv1d(512, 512, kernel_size=3, stride=1, padding=1, groups=512),\n",
    "            # nn.BatchNorm1d(512),\n",
    "            nn.Conv1d(512, 512, kernel_size=1),\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.LeakyReLU(inplace=True)\n",
    "        )\n",
    "        self.separable_conv8 = nn.Sequential(\n",
    "            nn.Conv1d(512, 512, kernel_size=3, stride=1, padding=1, groups=512),\n",
    "            # nn.BatchNorm1d(512),\n",
    "            nn.Conv1d(512, 512, kernel_size=1),\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.LeakyReLU(inplace=True)\n",
    "        )\n",
    "        self.separable_conv9 = nn.Sequential(\n",
    "            nn.Conv1d(512, 512, kernel_size=3, stride=1, padding=1, groups=512),\n",
    "            # nn.BatchNorm1d(512),\n",
    "            nn.Conv1d(512, 512, kernel_size=1),\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.LeakyReLU(inplace=True)\n",
    "        )\n",
    "        self.separable_conv10 = nn.Sequential(\n",
    "            nn.Conv1d(512, 1024, kernel_size=3, stride=2, padding=1, groups=512),\n",
    "            nn.Conv1d(1024, 1024, kernel_size=1),\n",
    "            nn.BatchNorm1d(1024),\n",
    "            nn.LeakyReLU(inplace=True)\n",
    "        )\n",
    "        self.separable_conv11 = nn.Sequential(\n",
    "            nn.Conv1d(1024, 1024, kernel_size=3, stride=2, padding=1, groups=1024),\n",
    "            nn.Conv1d(1024, 1024, kernel_size=1),\n",
    "            nn.BatchNorm1d(1024),\n",
    "            nn.LeakyReLU(inplace=True)\n",
    "        )\n",
    "        self.global_pool = nn.AdaptiveAvgPool1d(1)\n",
    "        self.classifier = nn.Linear(1024, num_classes)\n",
    "\n",
    "        self.apply(init_weights_he)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # print(x.shape)\n",
    "        # x = x.view(-1, 1, self.num_samples)\n",
    "        # print(\"after\", x.shape)\n",
    "\n",
    "        # Apply the convolutional layers\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = self.separable_conv1(x)\n",
    "        x = self.separable_conv2(x)\n",
    "        x = self.separable_conv3(x)\n",
    "        x = self.separable_conv4(x)\n",
    "        x = self.separable_conv5(x)\n",
    "        x = self.separable_conv6(x)\n",
    "        x = self.separable_conv7(x)\n",
    "        x = self.separable_conv8(x)\n",
    "        x = self.separable_conv9(x)\n",
    "        x = self.separable_conv10(x)\n",
    "        x = self.separable_conv11(x)\n",
    "        \n",
    "        # Global average pooling\n",
    "        x = self.global_pool(x)\n",
    "        \n",
    "        # Flatten the output\n",
    "        x = x.view(x.size(0), -1)\n",
    "        \n",
    "        # Classifier\n",
    "        x = self.classifier(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to calculate the number of parameters in the model\n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_loader,val_loader, criterion, optimizer, num_epochs=10):\n",
    "    train_loss_history = []\n",
    "    train_acc_history = []\n",
    "    val_loss_history = []\n",
    "    val_acc_history = []\n",
    "    \n",
    "    start_time = time.time()\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}\")\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "\n",
    "        for inputs, labels in train_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            outputs_softmax = F.softmax(outputs, dim=1)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "        \n",
    "        epoch_train_loss = running_loss / len(train_loader.dataset)\n",
    "        epoch_train_acc = correct / total\n",
    "        train_loss_history.append(epoch_train_loss)\n",
    "        train_acc_history.append(epoch_train_acc)\n",
    "\n",
    "        print(f\"Train Loss: {epoch_train_loss:.4f}, Train Accuracy: {epoch_train_acc:.4f}\")\n",
    "\n",
    "        # Validation\n",
    "        model.eval()\n",
    "        val_running_loss = 0.0\n",
    "        val_correct = 0\n",
    "        val_total = 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for val_inputs, val_labels in val_loader:\n",
    "                val_inputs, val_labels = val_inputs.to(device), val_labels.to(device)\n",
    "                val_outputs = model(val_inputs)\n",
    "                val_loss = criterion(val_outputs, val_labels)\n",
    "                val_running_loss += val_loss.item() * val_inputs.size(0)\n",
    "                _, val_predicted = torch.max(val_outputs, 1)\n",
    "                val_total += val_labels.size(0)\n",
    "                val_correct += (val_predicted == val_labels).sum().item()\n",
    "\n",
    "        epoch_val_loss = val_running_loss / len(val_loader.dataset)\n",
    "        epoch_val_acc = val_correct / val_total\n",
    "        val_loss_history.append(epoch_val_loss)\n",
    "        val_acc_history.append(epoch_val_acc)\n",
    "\n",
    "        print(f\"Validation Loss: {epoch_val_loss:.4f}, Validation Accuracy: {epoch_val_acc:.4f}\")\n",
    "\n",
    "    end_time = time.time()  # Record end time\n",
    "    training_time = end_time - start_time\n",
    "    print(f\"Training Time: {training_time:.2f} seconds\")\n",
    "\n",
    "    # Plotting\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(range(1, num_epochs + 1), train_loss_history, label='Train Loss')\n",
    "    plt.plot(range(1, num_epochs + 1), val_loss_history, label='Validation Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.title('Training and Validation Loss')\n",
    "\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(range(1, num_epochs + 1), train_acc_history, label='Train Accuracy')\n",
    "    plt.plot(range(1, num_epochs + 1), val_acc_history, label='Validation Accuracy')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.legend()\n",
    "    plt.title('Training and Validation Accuracy')\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "    # Calculate model size and number of parameters\n",
    "    model_size_mb = sum(p.numel() for p in model.parameters()) / (1024 * 1024)\n",
    "    num_parameters = count_parameters(model)\n",
    "    print(f\"Model Size: {model_size_mb:.2f} MB\")\n",
    "    print(f\"Number of Parameters: {num_parameters}\")\n",
    "\n",
    "    # Save the trained model\n",
    "    torch.save(model.state_dict(), 'YAMNETRawAudio_spec.pth')\n",
    "    torch.save(model, \"YAMNETRawAudio_spec.pt\")\n",
    "\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def evaluate_model(model, test_loader):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "    inference_start_time = time.time()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in test_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "            y_true.extend(labels.cpu().numpy())\n",
    "            y_pred.extend(predicted.cpu().numpy())\n",
    "\n",
    "    inference_end_time = time.time()\n",
    "    inference_time = inference_end_time - inference_start_time\n",
    "    print(f\"Inference Time: {inference_time:.4f} seconds\")\n",
    "    \n",
    "    test_accuracy = correct / total\n",
    "    print('Test Accuracy:', test_accuracy)\n",
    "\n",
    "    # Confusion matrix\n",
    "    conf_matrix = confusion_matrix(y_true, y_pred)\n",
    "    print(\"Confusion Matrix:\")\n",
    "    print(conf_matrix)\n",
    "\n",
    "    # Classification report\n",
    "    print(\"Classification Report:\")\n",
    "    print(classification_report(y_true, y_pred, target_names=test_dataset.classes))\n",
    "\n",
    "    # Plot confusion matrix\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', xticklabels=test_dataset.classes, yticklabels=test_dataset.classes)\n",
    "    plt.xlabel('Predicted Label')\n",
    "    plt.ylabel('True Label')\n",
    "    plt.title('Confusion Matrix (Test)')\n",
    "    plt.show()\n",
    "\n",
    "    # Plot classification report\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(pd.DataFrame.from_dict(classification_report(y_true, y_pred, target_names=test_dataset.classes, output_dict=True)), annot=True, cmap='Blues')\n",
    "    plt.xlabel('Metrics')\n",
    "    plt.ylabel('Classes')\n",
    "    plt.title('Classification Report (Test)')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bayesian optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class YAMNet(nn.Module):\n",
    "    def __init__(self, num_classes, num_samples, kernel_size):\n",
    "        super(YAMNet, self).__init__()\n",
    "        self.num_classes = num_classes\n",
    "        self.num_samples = num_samples\n",
    "\n",
    "        # Define the layers\n",
    "        self.conv1 = nn.Sequential(\n",
    "            nn.Conv1d(1, 32, kernel_size=kernel_size, stride=2, padding=1),\n",
    "            nn.BatchNorm1d(32, eps=1e-4),\n",
    "            nn.LeakyReLU(inplace=True)\n",
    "        )\n",
    "        self.separable_conv1 = nn.Sequential(\n",
    "            nn.Conv1d(32, 64, kernel_size=3, stride=1, padding=1, groups=32),\n",
    "            # nn.BatchNorm1d(32, eps=1e-4),\n",
    "            nn.Conv1d(64, 64, kernel_size=1),\n",
    "            nn.BatchNorm1d(64, eps=1e-4),\n",
    "            nn.LeakyReLU(inplace=True)\n",
    "        )\n",
    "        self.separable_conv2 = nn.Sequential(\n",
    "            nn.Conv1d(64, 128, kernel_size=3, stride=2, padding=1, groups=64),\n",
    "            # nn.BatchNorm1d(64, eps=1e-4),\n",
    "            nn.Conv1d(128, 128, kernel_size=1),\n",
    "            nn.BatchNorm1d(128, eps=1e-4),\n",
    "            nn.LeakyReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "        self.separable_conv3 = nn.Sequential(\n",
    "            nn.Conv1d(128, 128, kernel_size=3, stride=1, padding=1, groups=128),\n",
    "            nn.Conv1d(128, 128, kernel_size=1),\n",
    "            nn.BatchNorm1d(128, eps=1e-4),\n",
    "            nn.LeakyReLU(inplace=True)\n",
    "        )\n",
    "        self.separable_conv4 = nn.Sequential(\n",
    "            nn.Conv1d(128, 256, kernel_size=3, stride=2, padding=1, groups=128),\n",
    "            nn.Conv1d(256, 256, kernel_size=1),\n",
    "            nn.BatchNorm1d(256, eps=1e-4),\n",
    "            nn.LeakyReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "        self.separable_conv5 = nn.Sequential(\n",
    "            nn.Conv1d(256, 256, kernel_size=3, stride=1, padding=1, groups=256),\n",
    "            nn.Conv1d(256, 256, kernel_size=1),\n",
    "            nn.BatchNorm1d(256, eps=1e-4),\n",
    "            nn.LeakyReLU(inplace=True)\n",
    "        )\n",
    "        self.separable_conv6 = nn.Sequential(\n",
    "            nn.Conv1d(256, 512, kernel_size=3, stride=2, padding=1, groups=256),\n",
    "            # nn.BatchNorm1d(256),\n",
    "            nn.Conv1d(512, 512, kernel_size=1),\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.LeakyReLU(inplace=True)\n",
    "        )\n",
    "        self.separable_conv7 = nn.Sequential(\n",
    "            nn.Conv1d(512, 512, kernel_size=3, stride=1, padding=1, groups=512),\n",
    "            # nn.BatchNorm1d(512),\n",
    "            nn.Conv1d(512, 512, kernel_size=1),\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.LeakyReLU(inplace=True)\n",
    "        )\n",
    "        self.separable_conv8 = nn.Sequential(\n",
    "            nn.Conv1d(512, 512, kernel_size=3, stride=1, padding=1, groups=512),\n",
    "            # nn.BatchNorm1d(512),\n",
    "            nn.Conv1d(512, 512, kernel_size=1),\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.LeakyReLU(inplace=True)\n",
    "        )\n",
    "        self.separable_conv9 = nn.Sequential(\n",
    "            nn.Conv1d(512, 512, kernel_size=3, stride=1, padding=1, groups=512),\n",
    "            # nn.BatchNorm1d(512),\n",
    "            nn.Conv1d(512, 512, kernel_size=1),\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.LeakyReLU(inplace=True)\n",
    "        )\n",
    "        self.separable_conv10 = nn.Sequential(\n",
    "            nn.Conv1d(512, 1024, kernel_size=3, stride=2, padding=1, groups=512),\n",
    "            nn.Conv1d(1024, 1024, kernel_size=1),\n",
    "            nn.BatchNorm1d(1024),\n",
    "            nn.LeakyReLU(inplace=True)\n",
    "        )\n",
    "        self.separable_conv11 = nn.Sequential(\n",
    "            nn.Conv1d(1024, 1024, kernel_size=3, stride=2, padding=1, groups=1024),\n",
    "            nn.Conv1d(1024, 1024, kernel_size=1),\n",
    "            nn.BatchNorm1d(1024),\n",
    "            nn.LeakyReLU(inplace=True)\n",
    "        )\n",
    "        self.global_pool = nn.AdaptiveAvgPool1d(1)\n",
    "        self.classifier = nn.Linear(1024, num_classes)\n",
    "\n",
    "        self.apply(init_weights_he)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # print(x.shape)\n",
    "        # x = x.view(-1, 1, self.num_samples)\n",
    "        x=x.unsqueeze(1)\n",
    "        # print(\"after\", x.shape)\n",
    "\n",
    "        # Apply the convolutional layers\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = self.separable_conv1(x)\n",
    "        x = self.separable_conv2(x)\n",
    "        x = self.separable_conv3(x)\n",
    "        x = self.separable_conv4(x)\n",
    "        x = self.separable_conv5(x)\n",
    "        x = self.separable_conv6(x)\n",
    "        x = self.separable_conv7(x)\n",
    "        x = self.separable_conv8(x)\n",
    "        x = self.separable_conv9(x)\n",
    "        x = self.separable_conv10(x)\n",
    "        x = self.separable_conv11(x)\n",
    "        \n",
    "        # Global average pooling\n",
    "        x = self.global_pool(x)\n",
    "        \n",
    "        # Flatten the output\n",
    "        x = x.view(x.size(0), -1)\n",
    "        \n",
    "        # Classifier\n",
    "        x = self.classifier(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_model(model, val_loader):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in val_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    return correct / total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skopt.space import Real, Integer\n",
    "from skopt import gp_minimize\n",
    "from functools import partial\n",
    "\n",
    "# Define hyperparameter space for Bayesian optimization\n",
    "search_space = [\n",
    "                Integer(1, 4, name='kernel_size'),\n",
    "                Integer(1, 2, name='stride'),\n",
    "                ]\n",
    "\n",
    "sample_rate = 44100\n",
    "results_file = \"yam_optim.txt\"\n",
    "\n",
    "# Perform Bayesian optimization\n",
    "@use_named_args(search_space)\n",
    "def optimize_model(kernel_size, stride):\n",
    "    # Define model architecture and other necessary components\n",
    "    model = YAMNet(num_classes=len(train_dataset.classes),\n",
    "                   num_samples=desired_duration * sample_rate,\n",
    "                   kernel_size=kernel_size, stride=stride, padding=1).to(device)\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.0003)\n",
    "    \n",
    "    # Train the model\n",
    "    trained_model = train_model(model, train_loader, validation_loader, criterion, optimizer, num_epochs=50)\n",
    "    val_accuracy = validate_model(trained_model, validation_loader)\n",
    "    \n",
    "    print(f\"Learning Rate: 0.0003,kernel_size:{kernel_size},stride:{stride},padding:1, Validation Accuracy: {val_accuracy}\")\n",
    "\n",
    "    with open(results_file, 'a') as f:\n",
    "        f.write(f\"Learning Rate: 0.0003,kernel_size: {kernel_size}, stride:{stride},padding:1,Validation Accuracy: {val_accuracy}\\n\")\n",
    "    # Return the validation accuracy as the optimization target\n",
    "    return -val_accuracy \n",
    "\n",
    "# Set the number of optimization iterations\n",
    "n_calls = 20\n",
    "\n",
    "# Run the optimization\n",
    "res_gp = gp_minimize(partial(optimize_model),\n",
    "                     search_space,\n",
    "                     n_calls=n_calls,\n",
    "                     random_state=42)\n",
    "\n",
    "# Get best hyperparameters\n",
    "best_params = dict(zip(['kernel_size', 'stride',], res_gp.x))\n",
    "print(\"Best hyperparameters:\", best_params)\n",
    "\n",
    "# Train model with best hyperparameters\n",
    "best_accuracy = -res_gp.fun\n",
    "print(\"Best accuracy:\", best_accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = YAMNet(num_classes=3, num_samples=6*16000).to(device)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_summary(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialize model, criterion, and optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_gp.x[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0002, weight_decay = 0.0001)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_model(model, train_loader, validation_loader, criterion, optimizer, num_epochs=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_model(model, test_loader)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AI_front_end_new",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
