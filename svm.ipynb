{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Requiremets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torchaudio\n",
    "import torchvision.transforms as transforms\n",
    "import os\n",
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import torchvision.models as models\n",
    "import torch.nn.functional as F\n",
    "import time\n",
    "import librosa\n",
    "from skopt import gp_minimize\n",
    "from skopt.space import Real, Categorical\n",
    "from skopt.utils import use_named_args\n",
    "\n",
    "# Check for CUDA availability\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from collections import defaultdict\n",
    "\n",
    "\n",
    "class Add(nn.Module):\n",
    "    '''\n",
    "    Adds two tensors and returns the result\n",
    "    '''\n",
    "    def __init__(self,activation=None):\n",
    "        super(Add, self).__init__()\n",
    "        self.activation = activation\n",
    "        self.digital = True\n",
    "        \n",
    "    def forward(self, x):\n",
    "        if len(x) != 2:\n",
    "            print('ERR: Num tensors to add',len(x))\n",
    "            raise\n",
    "#         return torch.stack(x,dim=0).sum(dim=0)\n",
    "        if self.activation is not None:\n",
    "            return self.activation(torch.stack(x,dim=0).sum(dim=0))\n",
    "        else:\n",
    "            return torch.stack(x,dim=0).sum(dim=0)\n",
    "        \n",
    "def model_summary(M, pt_191=False):\n",
    "    \"\"\"\n",
    "    This function provides summary of all the named classes in the model.\n",
    "    Use arguments pt_191=True for pytorch 1.9.1 usage, default pt_191 = False\n",
    "    Returns a dictionary of class names and usage count.\n",
    "    \"\"\"\n",
    "    def zero(): return 0\n",
    "    cdict = defaultdict(zero)\n",
    "    \n",
    "\n",
    "    for n,m in M.named_modules(remove_duplicate=True):\n",
    "        if isinstance(m,nn.Conv2d):\n",
    "            if M.get_submodule(n.rsplit('.',1)[0]).__class__.__name__ == 'CART':\n",
    "                cdict['CART_'+m.__class__.__name__]+=1\n",
    "                \n",
    "            else:\n",
    "                cdict[m.__class__.__name__]+=1\n",
    "                \n",
    "            \n",
    "        elif isinstance(m,(nn.ReLU,Add)) and hasattr(m,'digital'):\n",
    "            if m.digital:\n",
    "                cdict[m.__class__.__name__]+=1\n",
    "                \n",
    "            else:\n",
    "                cdict['CART_'+m.__class__.__name__]+=1\n",
    "                \n",
    "        else:\n",
    "             cdict[m.__class__.__name__]+=1\n",
    "        \n",
    "            \n",
    "    w_size=0        \n",
    "    for p in M.parameters():\n",
    "        w_size+=p.shape.numel()\n",
    "    cdict['Parameters'] = str(w_size/1e6)+'M'   \n",
    "        \n",
    "    return dict(cdict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Class AudioDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AudioDataset(Dataset):\n",
    "    def __init__(self, directory, desired_duration, sample_rate=44100, n_mfcc=20):\n",
    "        self.directory = directory\n",
    "        self.classes = sorted(os.listdir(directory))\n",
    "        self.audio_files = []\n",
    "        self.desired_duration = desired_duration\n",
    "        self.sample_rate=sample_rate\n",
    "        self.n_mfcc=n_mfcc\n",
    "\n",
    "        for i, class_name in enumerate(self.classes):\n",
    "            class_path = os.path.join(directory, class_name)\n",
    "            for audio_file in os.listdir(class_path):\n",
    "                self.audio_files.append((os.path.join(class_path, audio_file), i))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.audio_files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        audio_file, label = self.audio_files[idx]\n",
    "        waveform, sample_rate = librosa.load(audio_file, sr=None) \n",
    "        mfcc = self._compute_mfcc(waveform)\n",
    "        return mfcc, label\n",
    "    \n",
    "    def _compute_mfcc(self, waveform):\n",
    "        if len(waveform) != self.desired_duration * self.sample_rate:\n",
    "            waveform = librosa.resample(waveform, orig_sr=len(waveform), target_sr=self.sample_rate)\n",
    "\n",
    "        if len(waveform) < self.desired_duration * self.sample_rate:\n",
    "            pad_size = self.desired_duration * self.sample_rate - len(waveform)\n",
    "            waveform = np.pad(waveform, (0, pad_size))\n",
    "        elif len(waveform) > self.desired_duration * self.sample_rate:\n",
    "            waveform = waveform[:self.desired_duration * self.sample_rate]\n",
    "\n",
    "        # Compute MFCC\n",
    "        mfcc = librosa.feature.mfcc(y=waveform, sr=self.sample_rate, n_mfcc=self.n_mfcc)\n",
    "\n",
    "        # Compute Mel spectrogram\n",
    "        mel_spectrogram = librosa.feature.melspectrogram(y=waveform, sr=self.sample_rate)\n",
    "\n",
    "        # Compute Chroma CQT\n",
    "        chroma_cqt = librosa.feature.chroma_cqt(y=waveform, sr=self.sample_rate)\n",
    "\n",
    "        # Stack the features\n",
    "        stacked_features = np.vstack([mfcc, mel_spectrogram, chroma_cqt])\n",
    "        return torch.tensor(stacked_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define data directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dir = 'data/train'\n",
    "validation_dir = 'data/validate'\n",
    "test_dir = 'data/test'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "desired_duration = 6  # Duration in seconds\n",
    "train_dataset = AudioDataset(train_dir, desired_duration=desired_duration)\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "validation_dataset = AudioDataset(validation_dir,desired_duration=desired_duration)\n",
    "validation_loader = DataLoader(validation_dataset, batch_size=8, shuffle=False)\n",
    "\n",
    "test_dataset = AudioDataset(test_dir, desired_duration=desired_duration)\n",
    "test_loader = DataLoader(test_dataset, batch_size=1, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Class SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "# Define SVM model\n",
    "class SVM(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SVM, self).__init__()\n",
    "        self.svm = SVC(kernel='rbf', gamma='scale')  # You can adjust kernel and other parameters here\n",
    "\n",
    "    def forward(self, x):\n",
    "        # SVM doesn't need forward pass as it's not a neural network\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to calculate the number of parameters in the model\n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_svm(model, train_loader):\n",
    "    # Flatten features and labels for SVM\n",
    "    X_train = []\n",
    "    y_train = []\n",
    "    for inputs, labels in train_loader:\n",
    "        X_train.append(inputs.view(inputs.size(0), -1).numpy())\n",
    "        y_train.extend(labels.numpy())\n",
    "\n",
    "    X_train = np.concatenate(X_train)\n",
    "    \n",
    "    # Train the SVM model\n",
    "    model.svm.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimization (optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "# Define the parameter grid\n",
    "param_grid = {\n",
    "    'C': [0.1, 1, 10, 100],  # Regularization parameter\n",
    "    'kernel': ['linear', 'poly', 'rbf', 'sigmoid'],  # Kernel function\n",
    "    'gamma': ['scale', 'auto', 0.1, 0.01, 0.001]  # Kernel coefficient for 'rbf', 'poly', and 'sigmoid' kernels\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm_model = SVC()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert DataLoader to numpy arrays\n",
    "X_train_all = []\n",
    "y_train_all = []\n",
    "for inputs, labels in train_loader:\n",
    "    X_train_all.append(inputs.view(inputs.size(0), -1).numpy())\n",
    "    y_train_all.extend(labels.numpy())\n",
    "\n",
    "X_train_all = np.concatenate(X_train_all)\n",
    "\n",
    "# Perform grid search with cross-validation\n",
    "grid_search = GridSearchCV(estimator=svm_model, param_grid=param_grid, cv=5, scoring='accuracy', verbose=1)\n",
    "grid_search.fit(X_train_all, y_train_all)\n",
    "\n",
    "# Get the best parameters and best accuracy\n",
    "best_params = grid_search.best_params_\n",
    "best_accuracy = grid_search.best_score_\n",
    "\n",
    "print(\"Best Parameters:\", best_params)\n",
    "print(\"Best Accuracy:\", best_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_rate = 44100\n",
    "model = SVM()\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_summary(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_svm(model, train_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_svm(model, val_loader):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "\n",
    "    for inputs, labels in val_loader:\n",
    "        inputs_flat = inputs.view(inputs.size(0), -1).numpy()\n",
    "        outputs = model.svm.predict(inputs_flat)\n",
    "        y_pred.extend(outputs)\n",
    "        y_true.extend(labels.numpy())\n",
    "        total += labels.size(0)\n",
    "        correct += (outputs == labels.numpy()).sum().item()\n",
    "\n",
    "    accuracy = correct / total\n",
    "    return accuracy, y_true, y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_accuracy, y_true_val, y_pred_val = validate_svm(model, validation_loader)\n",
    "print(\"Validation Accuracy:\", val_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_svm(model, test_loader):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "\n",
    "    for inputs, labels in test_loader:\n",
    "        inputs_flat = inputs.view(inputs.size(0), -1).numpy()\n",
    "        outputs = model.svm.predict(inputs_flat)\n",
    "        y_pred.extend(outputs)\n",
    "        y_true.extend(labels.numpy())\n",
    "        total += labels.size(0)\n",
    "        correct += (outputs == labels.numpy()).sum().item()\n",
    "\n",
    "    accuracy = correct / total\n",
    "    return accuracy, y_true, y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_accuracy, y_true_test, y_pred_test = test_svm(model, test_loader)\n",
    "print(\"Test Accuracy:\", test_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf_matrix = confusion_matrix(y_true_test, y_pred_test)\n",
    "\n",
    "# Print the confusion matrix\n",
    "print(\"Confusion Matrix:\")\n",
    "print(conf_matrix)\n",
    "\n",
    "# Plot the confusion matrix\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues')\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.ylabel('True Label')\n",
    "plt.title('Confusion Matrix (Test)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get class names\n",
    "class_names = test_dataset.classes\n",
    "\n",
    "# Plot the confusion matrix with class names\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', xticklabels=class_names, yticklabels=class_names)\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.ylabel('True Label')\n",
    "plt.title('Confusion Matrix (Test)')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Get true and predicted labels for the test set\n",
    "test_accuracy, y_true_test, y_pred_test = test_svm(model, test_loader)\n",
    "\n",
    "# Generate the classification report\n",
    "report = classification_report(y_true_test, y_pred_test)\n",
    "\n",
    "# Print the classification report\n",
    "print(\"Classification Report:\")\n",
    "print(report)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AI_front_end_new",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
