{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Requiremets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torchaudio\n",
    "import torchvision.transforms as transforms\n",
    "import os\n",
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import torchvision.models as models\n",
    "import torch.nn.functional as F\n",
    "import time\n",
    "import librosa\n",
    "from skopt import gp_minimize\n",
    "from skopt.space import Real, Categorical\n",
    "from skopt.utils import use_named_args\n",
    "\n",
    "# Check for CUDA availability\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from collections import defaultdict\n",
    "\n",
    "\n",
    "class Add(nn.Module):\n",
    "    '''\n",
    "    Adds two tensors and returns the result\n",
    "    '''\n",
    "    def __init__(self,activation=None):\n",
    "        super(Add, self).__init__()\n",
    "        self.activation = activation\n",
    "        self.digital = True\n",
    "        \n",
    "    def forward(self, x):\n",
    "        if len(x) != 2:\n",
    "            print('ERR: Num tensors to add',len(x))\n",
    "            raise\n",
    "#         return torch.stack(x,dim=0).sum(dim=0)\n",
    "        if self.activation is not None:\n",
    "            return self.activation(torch.stack(x,dim=0).sum(dim=0))\n",
    "        else:\n",
    "            return torch.stack(x,dim=0).sum(dim=0)\n",
    "        \n",
    "def model_summary(M, pt_191=False):\n",
    "    \"\"\"\n",
    "    This function provides summary of all the named classes in the model.\n",
    "    Use arguments pt_191=True for pytorch 1.9.1 usage, default pt_191 = False\n",
    "    Returns a dictionary of class names and usage count.\n",
    "    \"\"\"\n",
    "    def zero(): return 0\n",
    "    cdict = defaultdict(zero)\n",
    "    \n",
    "\n",
    "    for n,m in M.named_modules(remove_duplicate=True):\n",
    "        if isinstance(m,nn.Conv2d):\n",
    "            if M.get_submodule(n.rsplit('.',1)[0]).__class__.__name__ == 'CART':\n",
    "                cdict['CART_'+m.__class__.__name__]+=1\n",
    "                \n",
    "            else:\n",
    "                cdict[m.__class__.__name__]+=1\n",
    "                \n",
    "            \n",
    "        elif isinstance(m,(nn.ReLU,Add)) and hasattr(m,'digital'):\n",
    "            if m.digital:\n",
    "                cdict[m.__class__.__name__]+=1\n",
    "                \n",
    "            else:\n",
    "                cdict['CART_'+m.__class__.__name__]+=1\n",
    "                \n",
    "        else:\n",
    "             cdict[m.__class__.__name__]+=1\n",
    "        \n",
    "            \n",
    "    w_size=0        \n",
    "    for p in M.parameters():\n",
    "        w_size+=p.shape.numel()\n",
    "    cdict['Parameters'] = str(w_size/1e6)+'M'   \n",
    "        \n",
    "    return dict(cdict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Class AudioDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AudioDataset(Dataset):\n",
    "    def __init__(self, directory, desired_duration, sample_rate=44100):\n",
    "        self.directory = directory\n",
    "        self.classes = sorted(os.listdir(directory))\n",
    "        self.audio_files = []\n",
    "        self.desired_duration = desired_duration\n",
    "        self.sample_rate=sample_rate\n",
    "\n",
    "        for i, class_name in enumerate(self.classes):\n",
    "            class_path = os.path.join(directory, class_name)\n",
    "            for audio_file in os.listdir(class_path):\n",
    "                self.audio_files.append((os.path.join(class_path, audio_file), i))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.audio_files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        audio_file, label = self.audio_files[idx]\n",
    "        # print(f\"Loading audio file: {audio_file}\")\n",
    "        # waveform, sample_rate = torchaudio.load(audio_file)\n",
    "        waveform, sample_rate = librosa.load(audio_file, sr=None) \n",
    "        # print(f\"Loaded waveform shape: {waveform.shape}, Sample rate: {sample_rate}\")\n",
    "        waveform = self._process_waveform(waveform)\n",
    "        waveform = torch.tensor(waveform)\n",
    "        return waveform, label\n",
    "    \n",
    "    def _process_waveform(self, waveform):\n",
    "        if len(waveform) != self.desired_duration * self.sample_rate:\n",
    "            waveform = librosa.resample(waveform, orig_sr=len(waveform), target_sr=self.sample_rate)\n",
    "\n",
    "        if len(waveform) < self.desired_duration * self.sample_rate:\n",
    "            # print(\"Padding waveform...\")\n",
    "            pad_size = self.desired_duration * self.sample_rate - len(waveform)\n",
    "            waveform = torch.tensor(waveform).unsqueeze(0)  # Convert to torch tensor\n",
    "            waveform = torch.nn.functional.pad(waveform, (0, pad_size)).squeeze(0)  # Pad and remove the added dimension\n",
    "        elif len(waveform) > self.desired_duration * self.sample_rate:\n",
    "            # print(\"Truncating waveform...\")\n",
    "            waveform = waveform[:self.desired_duration * self.sample_rate]\n",
    "\n",
    "        return waveform"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define data directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dir = 'data/train'\n",
    "validation_dir = 'data/validate'\n",
    "test_dir = 'data/test'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "desired_duration = 6  # Duration in seconds\n",
    "train_dataset = AudioDataset(train_dir, desired_duration=desired_duration)\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "validation_dataset = AudioDataset(validation_dir,desired_duration=desired_duration)\n",
    "validation_loader = DataLoader(validation_dataset, batch_size=8, shuffle=False)\n",
    "\n",
    "test_dataset = AudioDataset(test_dir, desired_duration=desired_duration)\n",
    "test_loader = DataLoader(test_dataset, batch_size=1, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Class MobileNetV2RawAudio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class MobileNetV2RawAudio(nn.Module):\n",
    "    def __init__(self, num_classes, num_samples, dropout_rate=0.0, activation='ReLU'):\n",
    "        super(MobileNetV2RawAudio, self).__init__()\n",
    "        # Load MobileNetV2 model without the fully connected layer\n",
    "        self.num_classes=num_classes\n",
    "        self.num_samples = num_samples\n",
    "\n",
    "        self.features = models.mobilenet_v2(pretrained=True).features\n",
    "        self.features[0][0] = nn.Conv2d(1, 32, kernel_size=(3,3), stride=(2,2), padding=(1, 1), bias=False)\n",
    "    \n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Dropout(p=dropout_rate),\n",
    "            nn.Linear(1280, num_classes),\n",
    "            self.get_activation(activation) \n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        x = x.view(-1, 1, self.num_samples // 60, 60)\n",
    "        \n",
    "        # Apply the feature extractor layers\n",
    "        x = self.features(x)\n",
    "        \n",
    "        # Apply global average pooling\n",
    "        x = nn.functional.adaptive_avg_pool2d(x, (1, 1))\n",
    "        \n",
    "        # Flatten the tensor for the fully connected layers\n",
    "        x = torch.flatten(x, 1)\n",
    "        \n",
    "        # Apply the classifier layers\n",
    "        x = self.classifier(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "    def get_activation(self, activation):\n",
    "        if activation == 'ReLU':\n",
    "            return nn.ReLU(inplace=True)\n",
    "        elif activation == 'LeakyReLU':\n",
    "            return nn.LeakyReLU(inplace=True)\n",
    "        elif activation == 'Sigmoid':\n",
    "            return nn.Sigmoid()\n",
    "        else:\n",
    "            raise ValueError(\"Invalid activation function. Please choose from 'ReLU', 'LeakyReLU', or 'Sigmoid'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to calculate the number of parameters in the model\n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_loader,val_loader, criterion, optimizer, num_epochs=10):\n",
    "    train_loss_history = []\n",
    "    train_acc_history = []\n",
    "    val_loss_history = []\n",
    "    val_acc_history = []\n",
    "    \n",
    "    start_time = time.time()\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}\")\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "\n",
    "        for inputs, labels in train_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "        \n",
    "        epoch_train_loss = running_loss / len(train_loader.dataset)\n",
    "        epoch_train_acc = correct / total\n",
    "        train_loss_history.append(epoch_train_loss)\n",
    "        train_acc_history.append(epoch_train_acc)\n",
    "\n",
    "        print(f\"Train Loss: {epoch_train_loss:.4f}, Train Accuracy: {epoch_train_acc:.4f}\")\n",
    "\n",
    "        # Validation\n",
    "        model.eval()\n",
    "        val_running_loss = 0.0\n",
    "        val_correct = 0\n",
    "        val_total = 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for val_inputs, val_labels in val_loader:\n",
    "                val_inputs, val_labels = val_inputs.to(device), val_labels.to(device)\n",
    "                val_outputs = model(val_inputs)\n",
    "                val_loss = criterion(val_outputs, val_labels)\n",
    "                val_running_loss += val_loss.item() * val_inputs.size(0)\n",
    "                _, val_predicted = torch.max(val_outputs, 1)\n",
    "                val_total += val_labels.size(0)\n",
    "                val_correct += (val_predicted == val_labels).sum().item()\n",
    "\n",
    "        epoch_val_loss = val_running_loss / len(val_loader.dataset)\n",
    "        epoch_val_acc = val_correct / val_total\n",
    "        val_loss_history.append(epoch_val_loss)\n",
    "        val_acc_history.append(epoch_val_acc)\n",
    "\n",
    "        print(f\"Validation Loss: {epoch_val_loss:.4f}, Validation Accuracy: {epoch_val_acc:.4f}\")\n",
    "\n",
    "    end_time = time.time()  # Record end time\n",
    "    training_time = end_time - start_time\n",
    "    print(f\"Training Time: {training_time:.2f} seconds\")\n",
    "\n",
    "    # Plotting\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.subplot(2, 2, 1)\n",
    "    plt.plot(range(1, num_epochs + 1), train_loss_history, label='Train Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.title('Training Loss')\n",
    "\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.subplot(2, 2, 2)\n",
    "    plt.plot(range(1, num_epochs + 1), val_loss_history, label='Validation Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.title('Validation Loss')\n",
    "\n",
    "    plt.subplot(2, 2, 3)\n",
    "    plt.plot(range(1, num_epochs + 1), train_acc_history, label='Train Accuracy')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.legend()\n",
    "    plt.title('Training Accuracy')\n",
    "\n",
    "    plt.subplot(2, 2, 4)\n",
    "    plt.plot(range(1, num_epochs + 1), val_acc_history, label='Validation Accuracy')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.legend()\n",
    "    plt.title('Validation Accuracy')\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "    # Calculate model size and number of parameters\n",
    "    model_size_mb = sum(p.numel() for p in model.parameters()) / (1024 * 1024)\n",
    "    num_parameters = count_parameters(model)\n",
    "    print(f\"Model Size: {model_size_mb:.2f} MB\")\n",
    "    print(f\"Number of Parameters: {num_parameters}\")\n",
    "\n",
    "    # Save the trained model\n",
    "    torch.save(model.state_dict(), 'MobileNetV2RawAudio.pth')\n",
    "    torch.save(model, \"MobileNetV2RawAudio.pt\")\n",
    "\n",
    "    return model\n",
    "\n",
    "def evaluate_model(model, test_loader):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "    inference_start_time = time.time()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in test_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "            y_true.extend(labels.cpu().numpy())\n",
    "            y_pred.extend(predicted.cpu().numpy())\n",
    "\n",
    "    inference_end_time = time.time()\n",
    "    inference_time = inference_end_time - inference_start_time\n",
    "    print(f\"Inference Time: {inference_time:.4f} seconds\")\n",
    "    \n",
    "    test_accuracy = correct / total\n",
    "    print('Test Accuracy:', test_accuracy)\n",
    "\n",
    "    # Confusion matrix\n",
    "    conf_matrix = confusion_matrix(y_true, y_pred)\n",
    "    print(\"Confusion Matrix:\")\n",
    "    print(conf_matrix)\n",
    "\n",
    "    # Classification report\n",
    "    print(\"Classification Report:\")\n",
    "    print(classification_report(y_true, y_pred, target_names=test_dataset.classes))\n",
    "\n",
    "    # Plot confusion matrix\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', xticklabels=test_dataset.classes, yticklabels=test_dataset.classes)\n",
    "    plt.xlabel('Predicted Label')\n",
    "    plt.ylabel('True Label')\n",
    "    plt.title('Confusion Matrix (Test)')\n",
    "    plt.show()\n",
    "\n",
    "    # Plot classification report\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(pd.DataFrame.from_dict(classification_report(y_true, y_pred, target_names=test_dataset.classes, output_dict=True)), annot=True, cmap='Blues')\n",
    "    plt.xlabel('Metrics')\n",
    "    plt.ylabel('Classes')\n",
    "    plt.title('Classification Report (Test)')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bayesian optimization (optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_model(model, val_loader):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in val_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    return correct / total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define hyperparameter space for Bayesian optimization\n",
    "# search_space = [Real(1e-6, 1e-2, name='learning_rate')]\n",
    "search_space = [\n",
    "    Real(1e-6, 1e-2, name='learning_rate'),\n",
    "    Real(0.0, 0.5, name='dropout_rate'),\n",
    "    Categorical(['ReLU', 'LeakyReLU', 'Sigmoid'], name='activation')\n",
    "]\n",
    "sample_rate= 44100\n",
    "\n",
    "results_file = \"optimization_results_mobilenetv2.txt\"\n",
    "\n",
    "# Perform Bayesian optimization\n",
    "@use_named_args(search_space)\n",
    "def optimize_model(activation, learning_rate, dropout_rate):\n",
    "    # Define model architecture and other necessary components\n",
    "    model = MobileNetV2RawAudio(num_classes=len(train_dataset.classes),\n",
    "                                num_samples=desired_duration*sample_rate, \n",
    "                                dropout_rate=dropout_rate,\n",
    "                                activation=activation).to(device)\n",
    "    \n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    \n",
    "    # Train the model\n",
    "    trained_model = train_model(model, train_loader, validation_loader, criterion, optimizer, num_epochs=50)\n",
    "    val_accuracy = validate_model(trained_model, validation_loader)\n",
    "\n",
    "    print(f\"Learning Rate: {learning_rate},dropout_rate:{dropout_rate}, activation: {activation}, Validation Accuracy: {val_accuracy}\")\n",
    "\n",
    "    with open(results_file, 'a') as f:\n",
    "        f.write(f\"Learning Rate: {learning_rate}, Dropout Rate: {dropout_rate}, Activation: {activation}, Validation Accuracy: {val_accuracy}\\n\")\n",
    "    \n",
    "    \n",
    "    # Return the validation accuracy as the optimization target\n",
    "    return -val_accuracy \n",
    "\n",
    "res_gp = gp_minimize(optimize_model, search_space, n_calls=30, random_state=42)\n",
    "\n",
    "# Get best hyperparameters\n",
    "# best_params = dict(zip(['learning_rate'], res_gp.x))\n",
    "# print(\"Best hyperparameters:\", best_params)\n",
    "best_params = dict(zip(['learning_rate', 'dropout_rate','activation'], res_gp.x))\n",
    "print(\"Best hyperparameters:\", best_params)\n",
    "\n",
    "# Train model with best hyperparameters\n",
    "best_accuracy = -res_gp.fun\n",
    "print(\"Best accuracy:\", best_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MobileNetV2RawAudio(num_classes=len(train_dataset.classes),num_samples=desired_duration*sample_rate, dropout_rate = res_gp.x[1],activation=res_gp.x[2] ).to(device)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_summary(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialize model, criterion, and optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=res_gp.x[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_model(model, train_loader, validation_loader, criterion, optimizer, num_epochs=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_model(model, test_loader)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AI_front_end_new",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
