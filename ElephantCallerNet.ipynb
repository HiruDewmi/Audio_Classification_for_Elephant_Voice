{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Requiremets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torchaudio\n",
    "import torchvision.transforms as transforms\n",
    "import os\n",
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import torchvision.models as models\n",
    "import torch.nn.functional as F\n",
    "import time\n",
    "import librosa\n",
    "from skopt import gp_minimize\n",
    "from skopt.space import Real, Categorical, Integer\n",
    "from skopt.utils import use_named_args\n",
    "from torch.quantization import QuantStub, DeQuantStub\n",
    "\n",
    "# Check for CUDA availability\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from collections import defaultdict\n",
    "\n",
    "\n",
    "class Add(nn.Module):\n",
    "    '''\n",
    "    Adds two tensors and returns the result\n",
    "    '''\n",
    "    def __init__(self,activation=None):\n",
    "        super(Add, self).__init__()\n",
    "        self.activation = activation\n",
    "        self.digital = True\n",
    "        \n",
    "    def forward(self, x):\n",
    "        if len(x) != 2:\n",
    "            print('ERR: Num tensors to add',len(x))\n",
    "            raise\n",
    "#         return torch.stack(x,dim=0).sum(dim=0)\n",
    "        if self.activation is not None:\n",
    "            return self.activation(torch.stack(x,dim=0).sum(dim=0))\n",
    "        else:\n",
    "            return torch.stack(x,dim=0).sum(dim=0)\n",
    "        \n",
    "def model_summary(M, pt_191=False):\n",
    "    \"\"\"\n",
    "    This function provides summary of all the named classes in the model.\n",
    "    Use arguments pt_191=True for pytorch 1.9.1 usage, default pt_191 = False\n",
    "    Returns a dictionary of class names and usage count.\n",
    "    \"\"\"\n",
    "    def zero(): return 0\n",
    "    cdict = defaultdict(zero)\n",
    "    \n",
    "\n",
    "    for n,m in M.named_modules(remove_duplicate=True):\n",
    "        if isinstance(m,nn.Conv2d):\n",
    "            if M.get_submodule(n.rsplit('.',1)[0]).__class__.__name__ == 'CART':\n",
    "                cdict['CART_'+m.__class__.__name__]+=1\n",
    "                \n",
    "            else:\n",
    "                cdict[m.__class__.__name__]+=1\n",
    "                \n",
    "            \n",
    "        elif isinstance(m,(nn.ReLU,Add)) and hasattr(m,'digital'):\n",
    "            if m.digital:\n",
    "                cdict[m.__class__.__name__]+=1\n",
    "                \n",
    "            else:\n",
    "                cdict['CART_'+m.__class__.__name__]+=1\n",
    "                \n",
    "        else:\n",
    "             cdict[m.__class__.__name__]+=1\n",
    "        \n",
    "            \n",
    "    w_size=0        \n",
    "    for p in M.parameters():\n",
    "        w_size+=p.shape.numel()\n",
    "    cdict['Parameters'] = str(w_size/1e6)+'M'   \n",
    "        \n",
    "    return dict(cdict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Class AudioDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AudioDataset(Dataset):\n",
    "    def __init__(self, directory, desired_duration, sample_rate=44100):\n",
    "        self.directory = directory\n",
    "        self.classes = sorted(os.listdir(directory))\n",
    "        self.audio_files = []\n",
    "        self.desired_duration = desired_duration\n",
    "        self.sample_rate=sample_rate\n",
    "\n",
    "        for i, class_name in enumerate(self.classes):\n",
    "            class_path = os.path.join(directory, class_name)\n",
    "            for audio_file in os.listdir(class_path):\n",
    "                self.audio_files.append((os.path.join(class_path, audio_file), i))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.audio_files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        audio_file, label = self.audio_files[idx]\n",
    "        # print(f\"Loading audio file: {audio_file}\")\n",
    "        # waveform, sample_rate = torchaudio.load(audio_file)\n",
    "        waveform, sample_rate = librosa.load(audio_file, sr=None) \n",
    "        # print(f\"Loaded waveform shape: {waveform.shape}, Sample rate: {sample_rate}\")\n",
    "        waveform = self._process_waveform(waveform)\n",
    "        waveform = torch.tensor(waveform)\n",
    "        # label = F.one_hot(torch.tensor(label), num_classes=len(self.classes))\n",
    "        return waveform, label\n",
    "    \n",
    "    def _process_waveform(self, waveform):\n",
    "        if len(waveform) != self.desired_duration * self.sample_rate:\n",
    "            waveform = librosa.resample(waveform, orig_sr=len(waveform), target_sr=self.sample_rate)\n",
    "\n",
    "        if len(waveform) < self.desired_duration * self.sample_rate:\n",
    "            # print(\"Padding waveform...\")\n",
    "            pad_size = self.desired_duration * self.sample_rate - len(waveform)\n",
    "            waveform = torch.tensor(waveform).unsqueeze(0)  # Convert to torch tensor\n",
    "            waveform = torch.nn.functional.pad(waveform, (0, pad_size)).squeeze(0)  # Pad and remove the added dimension\n",
    "        elif len(waveform) > self.desired_duration * self.sample_rate:\n",
    "            # print(\"Truncating waveform...\")\n",
    "            waveform = waveform[:self.desired_duration * self.sample_rate]\n",
    "\n",
    "        return waveform"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define data directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dir = 'data/train'\n",
    "validation_dir = 'data/validate'\n",
    "test_dir = 'data/test'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "desired_duration = 6  # Duration in seconds\n",
    "train_dataset = AudioDataset(train_dir, desired_duration=desired_duration)\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "validation_dataset = AudioDataset(validation_dir,desired_duration=desired_duration)\n",
    "validation_loader = DataLoader(validation_dataset, batch_size=8, shuffle=False)\n",
    "\n",
    "test_dataset = AudioDataset(test_dir, desired_duration=desired_duration)\n",
    "test_loader = DataLoader(test_dataset, batch_size=1, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Class ElephantCallerNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tfeb_pool_size_component(length):\n",
    "    # print(length);\n",
    "    c = [];\n",
    "    index = 1;\n",
    "    while index <= 6:\n",
    "        if length >= 2:\n",
    "            if index == 6:\n",
    "                c.append(length);\n",
    "            else:\n",
    "                c.append(2);\n",
    "                length = length // 2;\n",
    "        else:\n",
    "           c.append(1);\n",
    "        index += 1;\n",
    "    return c\n",
    "\n",
    "def get_tfeb_pool_sizes(conv2_ch, width):\n",
    "    h = get_tfeb_pool_size_component(conv2_ch);\n",
    "    w = get_tfeb_pool_size_component(width);\n",
    "    # print(w);\n",
    "    pool_size = [];\n",
    "    for  (h1, w1) in zip(h, w):\n",
    "        pool_size.append((h1, w1));\n",
    "    return pool_size\n",
    "\n",
    "class ElephantCallerNet(nn.Module):\n",
    "    def __init__(self, input_length, n_class, sr, k_size, ch_conf=None, quantize=False):\n",
    "        super(ElephantCallerNet, self).__init__()\n",
    "        self.input_length = input_length\n",
    "        self.ch_config = ch_conf\n",
    "        self.quantize = quantize\n",
    "\n",
    "        stride1 = 2\n",
    "        stride2 = 2\n",
    "        channels = 8\n",
    "        k_size = k_size #(3, 3)\n",
    "        n_frames = (sr / 1000) * 10  # No of frames per 10ms\n",
    "\n",
    "        sfeb_pool_size = int(n_frames / (stride1 * stride2))\n",
    "        if self.ch_config is None:\n",
    "            self.ch_config = [channels, channels * 8, channels * 4, channels * 8, channels * 8,\n",
    "                              channels * 16, channels * 16, channels * 32, channels * 32, channels * 64,\n",
    "                              channels * 64, n_class]\n",
    "\n",
    "        fcn_no_of_inputs = self.ch_config[-1]\n",
    "\n",
    "        self.conv1, self.bn1 = self.make_layers(1, self.ch_config[0], (1, 9), (1, stride1))\n",
    "        self.conv2, self.bn2 = self.make_layers(self.ch_config[0], self.ch_config[1], (1, 5), (1, stride2))\n",
    "        self.conv3, self.bn3 = self.make_layers(1, self.ch_config[2], k_size, padding=1)\n",
    "        self.conv4, self.bn4 = self.make_layers(self.ch_config[2], self.ch_config[3], k_size, padding=1)\n",
    "        self.conv5, self.bn5 = self.make_layers(self.ch_config[3], self.ch_config[4], k_size, padding=1)\n",
    "        self.conv6, self.bn6 = self.make_layers(self.ch_config[4], self.ch_config[5], k_size, padding=1)\n",
    "        self.conv7, self.bn7 = self.make_layers(self.ch_config[5], self.ch_config[6], k_size, padding=1)\n",
    "        self.conv8, self.bn8 = self.make_layers(self.ch_config[6], self.ch_config[7], k_size, padding=1)\n",
    "        self.conv9, self.bn9 = self.make_layers(self.ch_config[7], self.ch_config[8], k_size, padding=1)\n",
    "        self.conv10, self.bn10 = self.make_layers(self.ch_config[8], self.ch_config[9], k_size, padding=1)\n",
    "        self.conv11, self.bn11 = self.make_layers(self.ch_config[9], self.ch_config[10], k_size, padding=1)\n",
    "        self.conv12, self.bn12 = self.make_layers(self.ch_config[10], self.ch_config[11], (1, 1))\n",
    "\n",
    "        self.fcn = nn.Linear(fcn_no_of_inputs, n_class)\n",
    "        nn.init.kaiming_normal_(self.fcn.weight, nonlinearity='sigmoid')\n",
    "\n",
    "        self.sfeb = nn.Sequential(\n",
    "            self.conv1, self.bn1, nn.ReLU(),\n",
    "            self.conv2, self.bn2, nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=(1, sfeb_pool_size))\n",
    "        )\n",
    "\n",
    "        tfeb_modules = []\n",
    "        self.tfeb_width = int(((self.input_length / sr) * 1000) / 10)  # 10ms frames of audio length in seconds\n",
    "        tfeb_pool_sizes = get_tfeb_pool_sizes(self.ch_config[1], self.tfeb_width)\n",
    "        p_index = 0\n",
    "        for i in [3, 4, 6, 8, 10]:\n",
    "            tfeb_modules.extend([eval('self.conv{}'.format(i)), eval('self.bn{}'.format(i)), nn.ReLU()])\n",
    "\n",
    "            if i != 3:\n",
    "                tfeb_modules.extend([eval('self.conv{}'.format(i + 1)), eval('self.bn{}'.format(i + 1)), nn.ReLU()])\n",
    "\n",
    "            h, w = tfeb_pool_sizes[p_index]\n",
    "            if h > 1 or w > 1:\n",
    "                tfeb_modules.append(nn.MaxPool2d(kernel_size=(max(1,h), max(1,w))))\n",
    "            p_index += 1\n",
    "\n",
    "        tfeb_modules.append(nn.Dropout(0.2))\n",
    "        tfeb_modules.extend([self.conv12, self.bn12, nn.ReLU()])\n",
    "        h, w = tfeb_pool_sizes[-1]\n",
    "        if h > 1 or w > 1:\n",
    "            tfeb_modules.append(nn.AvgPool2d(kernel_size=(max(1,h), max(1,w))))\n",
    "        tfeb_modules.extend([nn.Flatten(), self.fcn])\n",
    "\n",
    "        self.tfeb = nn.Sequential(*tfeb_modules)\n",
    "\n",
    "        self.output = nn.Sequential(\n",
    "            nn.Softmax(dim=1)\n",
    "        )\n",
    "\n",
    "        if self.quantize:\n",
    "            self.quant = QuantStub()\n",
    "            self.dequant = DeQuantStub()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.unsqueeze(0).unsqueeze(0)\n",
    "        if self.quantize:\n",
    "            x = self.quant(x)\n",
    "\n",
    "        x = self.sfeb(x)\n",
    "        x = x.permute((2, 0, 1, 3))\n",
    "        x = self.tfeb(x)\n",
    "\n",
    "        if self.quantize:\n",
    "            x = self.dequant(x)\n",
    "        y = self.output(x)\n",
    "        y = F.softmax(y, dim=1)\n",
    "        return y\n",
    "\n",
    "    def make_layers(self, in_channels, out_channels, kernel_size, stride=(1, 1), padding=0, bias=False):\n",
    "        conv = nn.Conv2d(in_channels=in_channels, out_channels=out_channels, kernel_size=kernel_size, stride=stride,\n",
    "                         padding=padding, bias=bias)\n",
    "        nn.init.kaiming_normal_(conv.weight, nonlinearity='relu')\n",
    "        bn = nn.BatchNorm2d(out_channels)\n",
    "        return conv, bn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to calculate the number of parameters in the model\n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_loader,val_loader, criterion, optimizer, num_epochs=10):\n",
    "    train_loss_history = []\n",
    "    train_acc_history = []\n",
    "    val_loss_history = []\n",
    "    val_acc_history = []\n",
    "    \n",
    "    start_time = time.time()\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}\")\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "\n",
    "        for inputs, labels in train_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "        \n",
    "        epoch_train_loss = running_loss / len(train_loader.dataset)\n",
    "        epoch_train_acc = correct / total\n",
    "        train_loss_history.append(epoch_train_loss)\n",
    "        train_acc_history.append(epoch_train_acc)\n",
    "\n",
    "        print(f\"Train Loss: {epoch_train_loss:.4f}, Train Accuracy: {epoch_train_acc:.4f}\")\n",
    "\n",
    "        # Validation\n",
    "        model.eval()\n",
    "        val_running_loss = 0.0\n",
    "        val_correct = 0\n",
    "        val_total = 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for val_inputs, val_labels in val_loader:\n",
    "                val_inputs, val_labels = val_inputs.to(device), val_labels.to(device)\n",
    "                val_outputs = model(val_inputs)\n",
    "                val_loss = criterion(val_outputs, val_labels)\n",
    "                val_running_loss += val_loss.item() * val_inputs.size(0)\n",
    "                _, val_predicted = torch.max(val_outputs, 1)\n",
    "                val_total += val_labels.size(0)\n",
    "                val_correct += (val_predicted == val_labels).sum().item()\n",
    "\n",
    "        epoch_val_loss = val_running_loss / len(val_loader.dataset)\n",
    "        epoch_val_acc = val_correct / val_total\n",
    "        val_loss_history.append(epoch_val_loss)\n",
    "        val_acc_history.append(epoch_val_acc)\n",
    "\n",
    "        print(f\"Validation Loss: {epoch_val_loss:.4f}, Validation Accuracy: {epoch_val_acc:.4f}\")\n",
    "\n",
    "    end_time = time.time()  # Record end time\n",
    "    training_time = end_time - start_time\n",
    "    print(f\"Training Time: {training_time:.2f} seconds\")\n",
    "\n",
    "    # Plotting\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.subplot(2, 2, 1)\n",
    "    plt.plot(range(1, num_epochs + 1), train_loss_history, label='Train Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.title('Training Loss')\n",
    "\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.subplot(2, 2, 2)\n",
    "    plt.plot(range(1, num_epochs + 1), val_loss_history, label='Validation Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.title('Validation Loss')\n",
    "\n",
    "    plt.subplot(2, 2, 3)\n",
    "    plt.plot(range(1, num_epochs + 1), train_acc_history, label='Train Accuracy')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.legend()\n",
    "    plt.title('Training Accuracy')\n",
    "\n",
    "    plt.subplot(2, 2, 4)\n",
    "    plt.plot(range(1, num_epochs + 1), val_acc_history, label='Validation Accuracy')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.legend()\n",
    "    plt.title('Validation Accuracy')\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "    # Calculate model size and number of parameters\n",
    "    model_size_mb = sum(p.numel() for p in model.parameters()) / (1024 * 1024)\n",
    "    num_parameters = count_parameters(model)\n",
    "    print(f\"Model Size: {model_size_mb:.2f} MB\")\n",
    "    print(f\"Number of Parameters: {num_parameters}\")\n",
    "\n",
    "    # Save the trained model\n",
    "    torch.save(model.state_dict(), 'adc_optim.pth')\n",
    "    torch.save(model, \"adc_optim.pt\")\n",
    "\n",
    "    return model\n",
    "\n",
    "def evaluate_model(model, test_loader):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "    inference_start_time = time.time()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in test_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "            y_true.extend(labels.cpu().numpy())\n",
    "            y_pred.extend(predicted.cpu().numpy())\n",
    "\n",
    "    inference_end_time = time.time()\n",
    "    inference_time = inference_end_time - inference_start_time\n",
    "    print(f\"Inference Time: {inference_time:.4f} seconds\")\n",
    "    \n",
    "    test_accuracy = correct / total\n",
    "    print('Test Accuracy:', test_accuracy)\n",
    "\n",
    "    # Confusion matrix\n",
    "    conf_matrix = confusion_matrix(y_true, y_pred)\n",
    "    print(\"Confusion Matrix:\")\n",
    "    print(conf_matrix)\n",
    "\n",
    "    # Classification report\n",
    "    print(\"Classification Report:\")\n",
    "    print(classification_report(y_true, y_pred, target_names=test_dataset.classes))\n",
    "\n",
    "    # Plot confusion matrix\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', xticklabels=test_dataset.classes, yticklabels=test_dataset.classes)\n",
    "    plt.xlabel('Predicted Label')\n",
    "    plt.ylabel('True Label')\n",
    "    plt.title('Confusion Matrix (Test)')\n",
    "    plt.show()\n",
    "\n",
    "    # Plot classification report\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(pd.DataFrame.from_dict(classification_report(y_true, y_pred, target_names=test_dataset.classes, output_dict=True)), annot=True, cmap='Blues')\n",
    "    plt.xlabel('Metrics')\n",
    "    plt.ylabel('Classes')\n",
    "    plt.title('Classification Report (Test)')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bayesian optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_model(model, val_loader):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in val_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    return correct / total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define hyperparameter space for Bayesian optimization\n",
    "# search_space = [Real(1e-6, 1e-2, name='learning_rate')]\n",
    "search_space = [\n",
    "    Real(1e-6, 1e-2, name='learning_rate'),\n",
    "    Integer(3, 7, name='kernel_size'),\n",
    "]\n",
    "sample_rate= 44100\n",
    "\n",
    "results_file = \"adc_optim.txt\"\n",
    "\n",
    "# Perform Bayesian optimization\n",
    "@use_named_args(search_space)\n",
    "def optimize_model(learning_rate, kernel_size):\n",
    "    # Define model architecture and other necessary components\n",
    "    model = ElephantCallerNet(n_class=len(train_dataset.classes),\n",
    "                                input_length=desired_duration*sample_rate, \n",
    "                                k_size=kernel_size, sr=sample_rate).to(device)\n",
    "    \n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    \n",
    "    # Train the model\n",
    "    trained_model = train_model(model, train_loader, validation_loader, criterion, optimizer, num_epochs=50)\n",
    "    val_accuracy = validate_model(trained_model, validation_loader)\n",
    "\n",
    "    print(f\"Learning Rate: {learning_rate},kernel_size:{kernel_size}, Validation Accuracy: {val_accuracy}\")\n",
    "\n",
    "    with open(results_file, 'a') as f:\n",
    "        f.write(f\"Learning Rate: {learning_rate},kernel_size: {kernel_size}, Validation Accuracy: {val_accuracy}\\n\")\n",
    "    \n",
    "    \n",
    "    # Return the validation accuracy as the optimization target\n",
    "    return -val_accuracy \n",
    "\n",
    "res_gp = gp_minimize(optimize_model, search_space, n_calls=30, random_state=42)\n",
    "\n",
    "# Get best hyperparameters\n",
    "# best_params = dict(zip(['learning_rate'], res_gp.x))\n",
    "# print(\"Best hyperparameters:\", best_params)\n",
    "best_params = dict(zip(['learning_rate', 'kernel_size'], res_gp.x))\n",
    "print(\"Best hyperparameters:\", best_params)\n",
    "\n",
    "# Train model with best hyperparameters\n",
    "best_accuracy = -res_gp.fun\n",
    "print(\"Best accuracy:\", best_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_rate= 44100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ElephantCallerNet(input_length=desired_duration*sample_rate, n_class=3, sr=sample_rate, quantize=False)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = MobileNetV2RawAudio(num_classes=len(train_dataset.classes),num_samples=desired_duration*sample_rate, dropout_rate = 0.4 ).to(device)\n",
    "# print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_summary(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialize model, criterion, and optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0005520881241955914)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_model(model.cuda(), train_loader, validation_loader, criterion, optimizer, num_epochs=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_model(model, test_loader)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AI_front_end_new",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
